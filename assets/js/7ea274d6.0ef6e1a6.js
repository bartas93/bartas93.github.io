"use strict";(self.webpackChunklearning_notes=self.webpackChunklearning_notes||[]).push([[5452],{79684:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>o,contentTitle:()=>s,default:()=>c,frontMatter:()=>r,metadata:()=>l,toc:()=>h});var t=a(85893),i=a(11151);const r={sidebar_label:"Fine Tuning",title:"Fine Tuning"},s=void 0,l={id:"cab420-machine-learning/fine-tuning",title:"Fine Tuning",description:"Deep networks need a lot of data to train. What can you do when you don't have much?",source:"@site/university/cab420-machine-learning/052-fine-tuning.mdx",sourceDirName:"cab420-machine-learning",slug:"/cab420-machine-learning/fine-tuning",permalink:"/university/cab420-machine-learning/fine-tuning",draft:!1,unlisted:!1,editUrl:"https://github.com/xiaohai-huang/learning-notes/tree/master/university/cab420-machine-learning/052-fine-tuning.mdx",tags:[],version:"current",lastUpdatedBy:"xiaohai-huang",lastUpdatedAt:1652408919,formattedLastUpdatedAt:"13 May 2022",sidebarPosition:52,frontMatter:{sidebar_label:"Fine Tuning",title:"Fine Tuning"},sidebar:"university",previous:{title:"ResNets",permalink:"/university/cab420-machine-learning/resnets"},next:{title:"Data Augmentation",permalink:"/university/cab420-machine-learning/data-augmentation"}},o={},h=[{value:"Keras Example",id:"keras-example",level:2},{value:"Preparation",id:"preparation",level:3},{value:"Fine-Tuning All Layers",id:"fine-tuning-all-layers",level:3},{value:"Fine-Tuning Only Some Layers",id:"fine-tuning-only-some-layers",level:3},{value:"Change From Classification to Regression",id:"change-from-classification-to-regression",level:3},{value:"Final Thoughts",id:"final-thoughts",level:2},{value:"References",id:"references",level:2}];function d(e){const n=Object.assign({p:"p",ul:"ul",li:"li",strong:"strong",h2:"h2",h3:"h3",pre:"pre",code:"code",admonition:"admonition",a:"a"},(0,i.ah)(),e.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:"Deep networks need a lot of data to train. What can you do when you don't have much?"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Fine Tuning, where we adapt one network to some new data"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Fine tuning will re-use a set of network weights for a new task.\nThis can be seen simply as starting with a much more advanced set of weights over random initialization."}),"\n",(0,t.jsxs)(n.p,{children:["When we fine tune, we may wish to remove or change some layers.\nThis may be because our new task has a different size output, or we've changed tasks entirely, going from for example ",(0,t.jsx)(n.strong,{children:"classification"})," to ",(0,t.jsx)(n.strong,{children:"regression"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"keras-example",children:"Keras Example"}),"\n",(0,t.jsx)(n.p,{children:"We will use a new set of data to fine-tune the network. This is just like compiling and fitting the network as per usual.\nBut there are a couple of things to note:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Generally, we will use a smaller learning rate for fine-tuning, the network is already trained after all.\nWe don't want to undo some of the good work of the old model."}),"\n",(0,t.jsx)(n.li,{children:"We often use SGD, as it will make small weights update."}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"preparation",children:"Preparation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Load a pre-trained model"',children:'model = keras.models.load_model("path/to/the/pre-trained-model.h5")\nmodel.summary()\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Load new data"',children:"(x_train, y_train), (x_test, y_test) = load_data()\n# reshape or normalized the data if necessary\n"})}),"\n",(0,t.jsx)(n.h3,{id:"fine-tuning-all-layers",children:"Fine-Tuning All Layers"}),"\n",(0,t.jsxs)(n.p,{children:["Use ",(0,t.jsx)(n.strong,{children:"SGD"})," optimizer with smaller learning rate."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Fine-Tuning all layers"',children:"model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              optimizer=keras.optimizers.SGD(lr=1e-4, momentum=0.9),\n              metrics=['accuracy'])\nmodel.fit(x_train, y_train,\n          batch_size=128,\n          epochs=10,\n          validation_data=(x_test, y_test))\n"})}),"\n",(0,t.jsx)(n.h3,{id:"fine-tuning-only-some-layers",children:"Fine-Tuning Only Some Layers"}),"\n",(0,t.jsx)(n.p,{children:"For very big networks (which we're not using), we don't really need to fine-tune everything -\nthe early layers are probably pretty good and we likely don't have enough data to tweak them in a meaningful way for example.\nAs such, we may want to restrict what we fine tune. We can do that really easily."}),"\n",(0,t.jsx)(n.p,{children:"In this example, we will just tune up the last 5 layers."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Only fine-tuning some layers"',children:"model = keras.models.load_model('../models/vgg_2stage_MNIST_small.h5')\n\n# Freeze the layers except the last 5 layers\nfor layer in model.layers[:-5]:\n    layer.trainable = False\n\n# Check the trainable status of the individual layers\nfor layer in model.layers:\n    print(layer, layer.trainable)\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              optimizer=keras.optimizers.SGD(lr=1e-4, momentum=0.9),\n              metrics=['accuracy'])\nmodel.fit(x_train, y_train,\n          batch_size=128,\n          epochs=20,\n          validation_data=(x_test, y_test))\n"})}),"\n",(0,t.jsx)(n.h3,{id:"change-from-classification-to-regression",children:"Change From Classification to Regression"}),"\n",(0,t.jsx)(n.p,{children:"Grab the output of a particular layer, and pass that to a new Dense layer, which we'll then pass to another Dense layer as output.\nFinally, we can create a model with the original model input, and the new model output."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'model = keras.models.load_model("model/path.h5")\nx = layers.Dense(64, activation="relu")(model.layers[-6].output)\noutputs = layers.Dense(1)(x)\n\nnew_model = Keras.Model(inputs=model.input, outputs=outputs)\nnew_model.summary()\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Train the new model"',children:"# train the model\nnew_model.compile(loss='mean_squared_error',\n              optimizer=keras.optimizers.SGD(learning_rate=1e-3, momentum=0.9))\n\nhistory = new_model.fit(x_train, y_train,\n                    batch_size=128,\n                    epochs=20,\n                    validation_data=(x_val, y_val))\n"})}),"\n",(0,t.jsx)(n.h2,{id:"final-thoughts",children:"Final Thoughts"}),"\n",(0,t.jsx)(n.p,{children:"Fine tuning can be seen as a form of advanced initialization. Rather than initialize our network with random weights, we instead initialize it with weights learned on a (hopefully) related task."}),"\n",(0,t.jsx)(n.p,{children:"The hope is that several layers of the network, in particular with CNNs the convolution layers that learn filters, will work well for the new task, and so the network can very quickly become accurate on the new data."}),"\n",(0,t.jsxs)(n.admonition,{type:"tip",children:[(0,t.jsx)(n.p,{children:"Often when fine tuning, there will be a need to replace some layers,\nparticularly for the network output, to adapt the network to the new task."}),(0,t.jsx)(n.p,{children:"This means that:"}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["these ",(0,t.jsx)(n.strong,{children:"new layers"})," will start from ",(0,t.jsx)(n.strong,{children:"random initializations"}),","]}),"\n",(0,t.jsxs)(n.li,{children:["the rest of the network is initialized with the ",(0,t.jsx)(n.strong,{children:"previously learned"})," weights."]}),"\n"]}),(0,t.jsxs)(n.p,{children:["In some cases, in particular where the network is very large, the tasks are quite similar,\nor data is very limited, a number of layers may be ",(0,t.jsx)(n.strong,{children:"frozen"})," such their weights are not trained at all."]}),(0,t.jsxs)(n.p,{children:["This can be particularly useful with deep DCNNs,\nwhere the early convolutional layers that learn ",(0,t.jsx)(n.strong,{children:"basic image primitives"})," that typically translate very well across datasets can be frozen to further reduce training effort."]})]}),"\n",(0,t.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://www.tensorflow.org/tutorials/images/transfer_learning",children:"Transfer learning and fine-tuning"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://github.com/xiaohai-huang/cab420-workspace/blob/master/work/machine-learning/week5/CAB420_DCNNs_Example_6_Fine_Tuning_and_Data_Augmentation.ipynb",children:"QUT Example"})}),"\n"]})]})}const c=function(e={}){const{wrapper:n}=Object.assign({},(0,i.ah)(),e.components);return n?(0,t.jsx)(n,Object.assign({},e,{children:(0,t.jsx)(d,e)})):d(e)}},11151:(e,n,a)=>{a.d(n,{Zo:()=>l,ah:()=>r});var t=a(67294);const i=t.createContext({});function r(e){const n=t.useContext(i);return t.useMemo((()=>"function"==typeof e?e(n):{...n,...e}),[n,e])}const s={};function l({components:e,children:n,disableParentContext:a}){let l;return l=a?"function"==typeof e?e({}):e||s:r(e),t.createElement(i.Provider,{value:l},n)}}}]);