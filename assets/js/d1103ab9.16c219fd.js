"use strict";(self.webpackChunklearning_notes=self.webpackChunklearning_notes||[]).push([[2752],{76734:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>a,metadata:()=>o,toc:()=>c});var t=s(85893),i=s(11151);const a={sidebar_label:"Gaussian Mixture Models",description:"A Gaussian mixture model assumes that each cluster has its own normal (or Gaussian) distribution with parameters \ud835\udf07\ud835\udc50 and \ud835\udf0e\ud835\udc50."},r="Gaussian Mixture Models (GMMs)",o={id:"cab420-machine-learning/gmms",title:"Gaussian Mixture Models (GMMs)",description:"A Gaussian mixture model assumes that each cluster has its own normal (or Gaussian) distribution with parameters \ud835\udf07\ud835\udc50 and \ud835\udf0e\ud835\udc50.",source:"@site/university/cab420-machine-learning/082-gmms.mdx",sourceDirName:"cab420-machine-learning",slug:"/cab420-machine-learning/gmms",permalink:"/university/cab420-machine-learning/gmms",draft:!1,unlisted:!1,editUrl:"https://github.com/xiaohai-huang/learning-notes/tree/master/university/cab420-machine-learning/082-gmms.mdx",tags:[],version:"current",lastUpdatedBy:"xiaohai-huang",lastUpdatedAt:1654143564,formattedLastUpdatedAt:"02 Jun 2022",sidebarPosition:82,frontMatter:{sidebar_label:"Gaussian Mixture Models",description:"A Gaussian mixture model assumes that each cluster has its own normal (or Gaussian) distribution with parameters \ud835\udf07\ud835\udc50 and \ud835\udf0e\ud835\udc50."},sidebar:"university",previous:{title:"K-Means",permalink:"/university/cab420-machine-learning/k-means"},next:{title:"Selection of K",permalink:"/university/cab420-machine-learning/how-to-select-k"}},l={},c=[{value:"Advantages",id:"advantages",level:2},{value:"Python",id:"python",level:2},{value:"References",id:"references",level:2}];function h(e){const n=Object.assign({h1:"h1",p:"p",em:"em",ul:"ul",li:"li",strong:"strong",admonition:"admonition",h2:"h2",pre:"pre",code:"code",img:"img",a:"a"},(0,i.ah)(),e.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"gaussian-mixture-models-gmms",children:"Gaussian Mixture Models (GMMs)"}),"\n",(0,t.jsxs)(n.p,{children:["GMM extends k-means, such that rather than having hard cluster boundary,\nwe now have a set of distributions, and we can capture ",(0,t.jsx)(n.em,{children:"uncertainty"}),".\nFor example, we'll be able to tell when a point is close to being part of two different classes."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A GMM will attempt to fit a set of Gaussian distributions to the data, and each Gaussian will capture one cluster (or component) of the data."}),"\n",(0,t.jsxs)(n.li,{children:["GMMs build upon K-means, using K-means for ",(0,t.jsx)(n.strong,{children:"initialization"})," before using ",(0,t.jsx)(n.strong,{children:"Expectation Maximization"})," to fit the model."]}),"\n"]}),"\n",(0,t.jsxs)(n.admonition,{type:"info",children:[(0,t.jsxs)(n.p,{children:["Unlike K-means, GMMs can capture different ",(0,t.jsx)(n.strong,{children:"shaped clusters"}),", and clusters ",(0,t.jsx)(n.strong,{children:"can overlap"}),"."]}),(0,t.jsx)(n.p,{children:"It can also measure how much a point belongs to multiple cluster centres and how likely a point is to arise from the distribution.\nThis has excellent applications for outlier/anomaly detection."})]}),"\n",(0,t.jsx)(n.h2,{id:"advantages",children:"Advantages"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Soft decisions"}),"\n",(0,t.jsx)(n.li,{children:"Likelihood"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["The big advantage of a GMM over k-means is that we get the ",(0,t.jsx)(n.strong,{children:"soft decisions"}),".\nK-means will assign a point to a cluster, and won't give any indication if there were multiple centres that could have fit that cluster."]}),"\n",(0,t.jsxs)(n.p,{children:["Secondly, a GMM will give us the ",(0,t.jsx)(n.strong,{children:"likelihood of a point"})," for each Gaussian,\nso we get a better idea of where the point lies, and how close it is to multiple cluster centres."]}),"\n",(0,t.jsxs)(n.admonition,{type:"info",children:[(0,t.jsxs)(n.p,{children:["In machine learning, we want to avoid ",(0,t.jsx)(n.strong,{children:"hard decisions"})," for as long as possible."]}),(0,t.jsxs)(n.p,{children:["K-means and it's assignment of points to a cluster can be seen as a ",(0,t.jsx)(n.strong,{children:"hard decision"}),". After we assign a point to a cluster, we ",(0,t.jsx)(n.em,{children:"lose"})," any other information about where that point lies."]}),(0,t.jsxs)(n.p,{children:["A GMM helps ",(0,t.jsx)(n.strong,{children:"overcome"})," this. Now we can do a ",(0,t.jsx)(n.strong,{children:"soft assignment"}),",\nand help carry forward the uncertainty about which cluster a point belongs to until we get further into our analysis."]})]}),"\n",(0,t.jsx)(n.h2,{id:"python",children:"Python"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from sklearn.mixture import GaussianMixture\ngmm = GaussianMixture(n_components=2)\ngmm.fit(X)\n"})}),"\n",(0,t.jsx)(n.p,{children:"We can get:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["the ",(0,t.jsx)(n.strong,{children:"means"})," of the components, i.e. the centers of each cluster."]}),"\n",(0,t.jsxs)(n.li,{children:["the ",(0,t.jsx)(n.strong,{children:"weights"})," of the components, i.e. the relative size of each cluster."]}),"\n",(0,t.jsxs)(n.li,{children:["the ",(0,t.jsx)(n.strong,{children:"covariances"})," of each cluster, this gives us the shape of the clusters."]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"print(gmm.means_)\nprint(gmm.weights_)\nprint(gmm.covariances_)\n"})}),"\n",(0,t.jsxs)(n.p,{children:["When assign points to clusters, we don't make a ",(0,t.jsx)(n.em,{children:"hard decision"}),". Rather we get the ",(0,t.jsx)(n.strong,{children:"likelihood"})," of the point arising from each component."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"print(gmm.predict_proba([[0.3,0.3]]))\nprint(gmm.predict_proba([[2.0,2.0]]))\n# below indicates that the first point belongs to the second class,\n# the second point belongs to the first class\n[[0.0064498 0.9935502]]\n[[0.99719157 0.00280843]]\n"})}),"\n",(0,t.jsx)(n.p,{children:"We can visualize this soft decision."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"gmm visualization",src:s(51322).Z+"",width:"603",height:"575"})}),"\n",(0,t.jsxs)(n.p,{children:["What we see here is that the two components ",(0,t.jsx)(n.strong,{children:"overlap"}),", so we have regions that points can belong to both clusters."]}),"\n",(0,t.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://github.com/xiaohai-huang/cab420-workspace/tree/master/work/machine-learning/week8",children:"Week8 Materials"})}),"\n"]})]})}const d=function(e={}){const{wrapper:n}=Object.assign({},(0,i.ah)(),e.components);return n?(0,t.jsx)(n,Object.assign({},e,{children:(0,t.jsx)(h,e)})):h(e)}},51322:(e,n,s)=>{s.d(n,{Z:()=>t});const t=s.p+"assets/images/gmms-visualization-2e46358efa9f5b230c3b16368b30aee9.png"},11151:(e,n,s)=>{s.d(n,{Zo:()=>o,ah:()=>a});var t=s(67294);const i=t.createContext({});function a(e){const n=t.useContext(i);return t.useMemo((()=>"function"==typeof e?e(n):{...n,...e}),[n,e])}const r={};function o({components:e,children:n,disableParentContext:s}){let o;return o=s?"function"==typeof e?e({}):e||r:a(e),t.createElement(i.Provider,{value:o},n)}}}]);